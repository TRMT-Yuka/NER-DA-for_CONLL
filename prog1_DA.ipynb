{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # MTG 1/6\n",
    "    #POStagとENTtagの間には何らかの関係性が存在？\n",
    "    #NERにおけるPOStagアプローチはsimpleと何も変わらないのでは＝＞誤差レベルの違いである可能性が高い\n",
    "    #構文木のほうにいけるとよいのですが..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data 拡張"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_original/train.txt', 'r', encoding='UTF-8') as f:\n",
    "    data = f.read()\n",
    "    data = data.split(\"\\n\\n\")\n",
    "    data = data[1:14987] #不要部分をカット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# storing conll format data in multiple list\n",
    "document = []\n",
    "for sentence in data:#14988 sentences\n",
    "    \n",
    "    new_sentence_list = []\n",
    "    sentence_list = sentence.split(\"\\n\")\n",
    "    \n",
    "    for token in sentence_list:\n",
    "        token_list = token.split(\" \")\n",
    "        new_sentence_list.append(token_list)\n",
    "    document.append(new_sentence_list)\n",
    "    \n",
    "    # [[['-DOCSTART-', '-X-', '-X-', 'O']],\n",
    "    #  [['EU', 'NNP', 'B-NP', 'B-ORG'],\n",
    "    #   ['rejects', 'VBZ', 'B-VP', 'O'],\n",
    "    #   ['German', 'JJ', 'B-NP', 'B-MISC'],..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new label\n",
    "new_document = []\n",
    "for sentence in document:\n",
    "    new_sentence_list = []\n",
    "    for token_list in sentence:\n",
    "        try:\n",
    "            new_token_list = [token_list[0],token_list[1]+\"_\"+token_list[3],token_list[3]]\n",
    "            new_sentence_list.append(new_token_list)\n",
    "        except:\n",
    "            pass\n",
    "    new_document.append(new_sentence_list)\n",
    "    \n",
    "# [[['EU', 'NNP_B-ORG', 'B-ORG'],\n",
    "#   ['rejects', 'VBZ_O', 'O'],\n",
    "#   ['German', 'JJ_B-MISC', 'B-MISC'],\n",
    "#   ['call', 'NN_O', 'O'],..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ここで一つの疑問\n",
    "    # 交換の割合はどのようにすべきだろうか？\n",
    "    # (1)simple augmentを参照し確認\n",
    "    # (2)とりあえず10% 30% 50% 70% 100%と順に試してみる => 先に作成してみましょ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dictonary object with label as key and token as value\n",
    "label_token_dic = {}\n",
    "for sentence in new_document:\n",
    "    for token_list in sentence:\n",
    "        label = token_list[1]\n",
    "        token = token_list[0]\n",
    "        if label not in label_token_dic:\n",
    "            label_token_dic[label] = set()\n",
    "        label_token_dic[label].add(token)\n",
    "        \n",
    "    #  {'NNP_B-ORG': {'Detroit',\n",
    "    #   'FK',\n",
    "    #   'ATRIA',\n",
    "    #   'DBRS',...\n",
    "    \n",
    "    # label_token_dic.keys() でkey一覧取得可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 交換可能パタン\n",
    "    # B，Iのみ　=> 固有表現に限定したsimple data augmantation\n",
    "    # oも含む 　=> アノテーションラベル全体に範囲を広げたsimple data augmantation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ひとまずB，Iのみから作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making BI label list\n",
    "BI_label_list = [label for label in list(label_token_dic.keys()) if \"B-\" in label or \"I-\" in label]\n",
    "    # ['NNP_B-ORG',\n",
    "    #  'JJ_B-MISC',\n",
    "    #  'NNP_B-PER',\n",
    "    #  'NNP_I-PER',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sent2str(sentence_list):\n",
    "    return \" \".join([token_list[0] for token_list in sentence_list])\n",
    "\n",
    "def random_choice_token(token_set):\n",
    "    n = random.randint(0,len(token_set)-1)\n",
    "    token_list=list(token_set)\n",
    "    return token_list[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "\n",
    "\n",
    "def SimpleDA_BI(n,filename):\n",
    "    data_f_path = \"data_augment/SimpleDA_BI/\"+str(n)+\"_\"+filename+\".txt\"\n",
    "    memo_f_path = \"data_augment/SimpleDA_BI/\"+str(n)+\"_\"+filename+\"_memo.txt\"\n",
    "    with open(data_f_path,\"w\") as data_f, open(memo_f_path,\"w\") as memo_f:\n",
    "        for now in range(n):\n",
    "            if now%10 == 0:\n",
    "                print('\\r%d / %d' %(now, n), end='')\n",
    "\n",
    "            x = random.randint(0,len(new_document)-1)\n",
    "            origin_sentence = new_document[x]\n",
    "            aug_sentence = []\n",
    "            cnt = 0\n",
    "\n",
    "            for token_list in origin_sentence:\n",
    "                token = token_list[0]\n",
    "                label = token_list[1]\n",
    "                origin_label = token_list[2]\n",
    "\n",
    "                if label in BI_label_list:\n",
    "                    if len(label_token_dic[label])!=1:\n",
    "                        token=random_choice_token(label_token_dic[label]-{token})\n",
    "                        cnt = cnt + 1\n",
    "                    \n",
    "\n",
    "                aug_sentence.append([token,origin_label])\n",
    "                data_f.write(\" \".join([token,origin_label])+\"\\n\")\n",
    "            data_f.write(\"\\n\")\n",
    "\n",
    "            memo_f.write(str(x)+\"\\t\"+Sent2str(new_document[x])+\"\\n\")\n",
    "            memo_f.write(\" \"*len(str(x))+\"\\t\"+Sent2str(aug_sentence)+\"\\n\")\n",
    "            memo_f.write(\"\\n\")\n",
    "        \n",
    "        memo_f.write(str(cnt)+\"sentences augmented!!!! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #train\n",
    "n = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1484990 / 1485000"
     ]
    }
   ],
   "source": [
    "SimpleDA_BI(n*1,\"train\")\n",
    "SimpleDA_BI(n*4,\"train\")\n",
    "SimpleDA_BI(n*9,\"train\")\n",
    "SimpleDA_BI(n*49,\"train\")\n",
    "SimpleDA_BI(n*99,\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## valid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_original/valid.txt', 'r', encoding='UTF-8') as f:\n",
    "    data = f.read()\n",
    "    data = data.split(\"\\n\\n\")\n",
    "    data = data[1:3466]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下同様\n",
    "document = []\n",
    "for sentence in data:#14988 sentences\n",
    "    \n",
    "    new_sentence_list = []\n",
    "    sentence_list = sentence.split(\"\\n\")\n",
    "    \n",
    "    for token in sentence_list:\n",
    "        token_list = token.split(\" \")\n",
    "        new_sentence_list.append(token_list)\n",
    "    document.append(new_sentence_list)\n",
    "\n",
    "new_document = []\n",
    "for sentence in document:\n",
    "    new_sentence_list = []\n",
    "    for token_list in sentence:\n",
    "        try:\n",
    "            new_token_list = [token_list[0],token_list[1]+\"_\"+token_list[3],token_list[3]]\n",
    "            new_sentence_list.append(new_token_list)\n",
    "        except:\n",
    "            pass\n",
    "    new_document.append(new_sentence_list)\n",
    "\n",
    "label_token_dic = {}\n",
    "for sentence in new_document:\n",
    "    for token_list in sentence:\n",
    "        label = token_list[1]\n",
    "        token = token_list[0]\n",
    "        if label not in label_token_dic:\n",
    "            label_token_dic[label] = set()\n",
    "        label_token_dic[label].add(token)\n",
    "\n",
    "        BI_label_list = [label for label in list(label_token_dic.keys()) if \"B-\" in label or \"I-\" in label]\n",
    "\n",
    "BI_label_list = [label for label in list(label_token_dic.keys()) if \"B-\" in label or \"I-\" in label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid\n",
    "n = 3500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346490 / 346500"
     ]
    }
   ],
   "source": [
    "SimpleDA_BI(n*1,\"valid\")\n",
    "SimpleDA_BI(n*4,\"valid\")\n",
    "SimpleDA_BI(n*9,\"valid\")\n",
    "SimpleDA_BI(n*49,\"valid\")\n",
    "SimpleDA_BI(n*99,\"valid\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
