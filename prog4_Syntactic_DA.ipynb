{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # MTG 1/6\n",
    "    #POStagとENTtagの間には何らかの関係性が存在？\n",
    "    #NERにおけるPOStagアプローチはsimpleと何も変わらないのでは＝＞誤差レベルの違いである可能性が高い\n",
    "    #構文木のほうにいけるとよいのですが..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data 拡張"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_original/train.txt', 'r', encoding='UTF-8') as f:\n",
    "    data = f.read()\n",
    "    data = data.split(\"\\n\\n\")\n",
    "    data = data[1:14987] #不要部分をカット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# storing conll format data in multiple list\n",
    "document = []\n",
    "for sentence in data:#14988 sentences\n",
    "    \n",
    "    new_sentence_list = []\n",
    "    sentence_list = sentence.split(\"\\n\")\n",
    "    \n",
    "    for token in sentence_list:\n",
    "        token_list = token.split(\" \")\n",
    "        new_sentence_list.append(token_list)\n",
    "    document.append(new_sentence_list)\n",
    "    \n",
    "    # [[['-DOCSTART-', '-X-', '-X-', 'O']],\n",
    "    #  [['EU', 'NNP', 'B-NP', 'B-ORG'],\n",
    "    #   ['rejects', 'VBZ', 'B-VP', 'O'],\n",
    "    #   ['German', 'JJ', 'B-NP', 'B-MISC'],..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['B-ADVP', 'I-ADVP', 'I-ADJP', 'I-SBAR',\n",
    "#  'I-PP', 'B-LST', 'B-SBAR', 'B-NP',\n",
    "#  'B-ADJP', 'I-INTJ', 'B-VP', '-X-',\n",
    "#  'I-CONJP', 'B-PRT', 'B-CONJP', 'I-LST',\n",
    "#  'B-INTJ', 'B-PP', 'I-VP', 'O', 'I-NP']\n",
    "\n",
    "# I-B I-other cut\n",
    "#    I-I stay \n",
    "# B-I\n",
    "#   B-B B-oter cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_document = []\n",
    "\n",
    "for sentence in document:\n",
    "    sentence_list = []\n",
    "    \n",
    "    for i in range(len(sentence)-1):\n",
    "        sentence_list.append(sentence[i])\n",
    "        \n",
    "        if \"I-\" in sentence[i][2]:\n",
    "            if \"I-\" in sentence[i+1][2]:\n",
    "                pass\n",
    "            else: #B,other\n",
    "                sentence_list.append(-1)\n",
    "                \n",
    "        else: #B,other\n",
    "            if \"I-\" in sentence[i+1][2]:\n",
    "                pass\n",
    "            else: #B,other\n",
    "                sentence_list.append(-1)\n",
    "    \n",
    "    final_sentence_list = []\n",
    "    new_tokens_list = []\n",
    "    all_token = []\n",
    "    syntactic_tag = \"\"\n",
    "    bio_tag = []\n",
    "    \n",
    "    for token in sentence_list:\n",
    "        if token == -1 or token==sentence_list[-1]:\n",
    "            \n",
    "            new_tokens_list.append(tuple(all_token))\n",
    "            new_tokens_list.append(tuple([syntactic_tag,tuple(bio_tag)]))#タグ生成\n",
    "            \n",
    "            final_sentence_list.append(new_tokens_list)\n",
    "            \n",
    "            new_tokens_list = []\n",
    "            all_token = []\n",
    "            syntactic_tag = \"\"\n",
    "            bio_tag = []\n",
    "        else:\n",
    "            all_token.append(token[0])\n",
    "            if syntactic_tag == \"\":\n",
    "                syntactic_tag=token[2]#タグ生成\n",
    "            bio_tag.append(token[3])\n",
    "    new_document.append(final_sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[('EU',), ('B-NP', ('B-ORG',))],\n",
       "  [('rejects',), ('B-VP', ('O',))],\n",
       "  [('German', 'call'), ('B-NP', ('B-MISC', 'O'))],\n",
       "  [('to', 'boycott'), ('B-VP', ('O', 'O'))],\n",
       "  [('British', 'lamb'), ('B-NP', ('B-MISC', 'O'))]],\n",
       " [[(), ('', ())]]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_document[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# とりあえず，一ブロックに一単語のみ格納されているケースは削除\n",
    "# 2以上のケース　かつ　ブロック内のBOIタグ構造が完全に一致するものだけを交換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dictonary object with label as key and token as value\n",
    "label_token_dic = {}\n",
    "cnt = 0\n",
    "for sentence in new_document:\n",
    "    for block in sentence:\n",
    "        if len(block[0])!=1:# block内にトークン2以上の時のみ辞書登録\n",
    "            tokens = block[0]\n",
    "            label = block[1]\n",
    "            cnt = cnt + 1\n",
    "            if label not in label_token_dic:\n",
    "                label_token_dic[label] = set()\n",
    "            label_token_dic[label].add(tuple(tokens))\n",
    "        \n",
    "    # {('B-NP', ('B-MISC', 'O')): {('A$', '1'),\n",
    "    #   ('A$', '100'),\n",
    "    #   ('A$', '14,000'),\n",
    "    #   ('A$', '28,000'),...\n",
    "    \n",
    "    # label_token_dic.keys() でkey一覧取得可　個数：663\n",
    "    # cnt　に合計要素数格納　個数：41377"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making BI label list\n",
    "# BIを含むもののみ交換\n",
    "\n",
    "BI_label_list = []\n",
    "for label in list(label_token_dic.keys()):\n",
    "    NG_set = [{\"O\"},set(),{\"-X-\"},{\"O\",\"-X-\"}]\n",
    "    if set(label[1]) not in NG_set:\n",
    "        BI_label_list.append(label)\n",
    "        \n",
    "# [('B-NP', ('B-MISC', 'O')),\n",
    "#  ('B-NP', ('O', 'B-ORG', 'I-ORG')),\n",
    "#  ('B-NP', ('O', 'O', 'O', 'B-PER', 'I-PER')),..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sent2str(sentence_list):\n",
    "    return \" \".join([token_list[0] for token_list in sentence_list])\n",
    "\n",
    "def Sent2str_for_syntactic(sentence_list):\n",
    "    sentence_str = \"\"\n",
    "    for block in sentence_list:\n",
    "        for token in block[0]:\n",
    "            sentence_str = sentence_str+\" \"+token\n",
    "    return sentence_str\n",
    "\n",
    "def random_choice_token(token_set):\n",
    "    n = random.randint(0,len(token_set)-1)\n",
    "    token_list=list(token_set)\n",
    "    return token_list[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "\n",
    "\n",
    "def SyntacticDA_BI(n,filename):\n",
    "    data_f_path = \"data_augment/SyntacticDA_BI/\"+str(n)+\"_\"+filename+\".txt\"\n",
    "    memo_f_path = \"data_augment/SyntacticDA_BI/\"+str(n)+\"_\"+filename+\"_memo.txt\"\n",
    "    with open(data_f_path,\"w\") as data_f, open(memo_f_path,\"w\") as memo_f:\n",
    "        for now in range(n):\n",
    "            if now%10 == 0:\n",
    "                print('\\r%d / %d' %(now, n), end='')\n",
    "\n",
    "            x = random.randint(0,len(new_document)-1)\n",
    "            origin_sentence = new_document[x]\n",
    "            aug_sentence = []\n",
    "            cnt = 0\n",
    "\n",
    "            for block_list in origin_sentence:\n",
    "                tokens = block_list[0]\n",
    "                label = block_list[1]\n",
    "                origin_labels = block_list[1][1]\n",
    "\n",
    "                if label in BI_label_list:\n",
    "                    if len(label_token_dic[label])!=1:\n",
    "                        tokens=random_choice_token(label_token_dic[label]-{tokens})\n",
    "                        cnt = cnt + 1\n",
    "                    \n",
    "                for i in range(len(tokens)):\n",
    "                    aug_sentence.append([tokens[i],origin_labels[i]])\n",
    "                    data_f.write(\" \".join([tokens[i],origin_labels[i]])+\"\\n\")\n",
    "            data_f.write(\"\\n\")\n",
    "            \\\n",
    "            memo_f.write(str(x)+\"\\t\"+Sent2str_for_syntactic(new_document[x])+\"\\n\")\n",
    "            memo_f.write(\" \"*len(str(x))+\"\\t\"+Sent2str(aug_sentence)+\"\\n\")\n",
    "            memo_f.write(\"\\n\")\n",
    "        \n",
    "        memo_f.write(str(cnt)+\"sentences augmented!!!! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #train\n",
    "n = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134990 / 135000"
     ]
    }
   ],
   "source": [
    "SyntacticDA_BI(n*1,\"train\")\n",
    "SyntacticDA_BI(n*4,\"train\")\n",
    "SyntacticDA_BI(n*9,\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## valid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_original/valid.txt', 'r', encoding='UTF-8') as f:\n",
    "    data = f.read()\n",
    "    data = data.split(\"\\n\\n\")\n",
    "    data = data[1:3466]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下同様\n",
    "document = []\n",
    "for sentence in data:#14988 sentences\n",
    "    \n",
    "    new_sentence_list = []\n",
    "    sentence_list = sentence.split(\"\\n\")\n",
    "    \n",
    "    for token in sentence_list:\n",
    "        token_list = token.split(\" \")\n",
    "        new_sentence_list.append(token_list)\n",
    "    document.append(new_sentence_list)\n",
    "\n",
    "\n",
    "\n",
    "new_document = []\n",
    "for sentence in document:\n",
    "    sentence_list = []\n",
    "    \n",
    "    for i in range(len(sentence)-1):\n",
    "        sentence_list.append(sentence[i])\n",
    "        \n",
    "        if \"I-\" in sentence[i][2]:\n",
    "            if \"I-\" in sentence[i+1][2]:\n",
    "                pass\n",
    "            else: #B,other\n",
    "                sentence_list.append(-1)\n",
    "                \n",
    "        else: #B,other\n",
    "            if \"I-\" in sentence[i+1][2]:\n",
    "                pass\n",
    "            else: #B,other\n",
    "                sentence_list.append(-1)\n",
    "    \n",
    "    final_sentence_list = []\n",
    "    new_tokens_list = []\n",
    "    all_token = []\n",
    "    syntactic_tag = \"\"\n",
    "    bio_tag = []\n",
    "    \n",
    "    for token in sentence_list:\n",
    "        if token == -1 or token==sentence_list[-1]:\n",
    "            \n",
    "            new_tokens_list.append(tuple(all_token))\n",
    "            new_tokens_list.append(tuple([syntactic_tag,tuple(bio_tag)]))#タグ生成\n",
    "            \n",
    "            final_sentence_list.append(new_tokens_list)\n",
    "            \n",
    "            new_tokens_list = []\n",
    "            all_token = []\n",
    "            syntactic_tag = \"\"\n",
    "            bio_tag = []\n",
    "        else:\n",
    "            all_token.append(token[0])\n",
    "            if syntactic_tag == \"\":\n",
    "                syntactic_tag=token[2]#タグ生成\n",
    "            bio_tag.append(token[3])\n",
    "    new_document.append(final_sentence_list)\n",
    "\n",
    "    \n",
    "\n",
    "label_token_dic = {}\n",
    "cnt = 0\n",
    "for sentence in new_document:\n",
    "    for block in sentence:\n",
    "        if len(block[0])!=1:# block内にトークン2以上の時のみ辞書登録\n",
    "            tokens = block[0]\n",
    "            label = block[1]\n",
    "            cnt = cnt + 1\n",
    "            if label not in label_token_dic:\n",
    "                label_token_dic[label] = set()\n",
    "            label_token_dic[label].add(tuple(tokens))\n",
    "\n",
    "\n",
    "BI_label_list = []\n",
    "for label in list(label_token_dic.keys()):\n",
    "    NG_set = [{\"O\"},set(),{\"-X-\"},{\"O\",\"-X-\"}]\n",
    "    if set(label[1]) not in NG_set:\n",
    "        BI_label_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid\n",
    "n = 3500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31490 / 31500"
     ]
    }
   ],
   "source": [
    "SyntacticDA_BI(n*1,\"valid\")\n",
    "SyntacticDA_BI(n*4,\"valid\")\n",
    "SyntacticDA_BI(n*9,\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
