{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # MTG 1/6\n",
    "    #POStagとENTtagの間には何らかの関係性が存在？\n",
    "    #NERにおけるPOStagアプローチはsimpleと何も変わらないのでは＝＞誤差レベルの違いである可能性が高い\n",
    "    #構文木のほうにいけるとよいのですが..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data 拡張"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_original/train.txt', 'r', encoding='UTF-8') as f:\n",
    "    data = f.read()\n",
    "    data = data.split(\"\\n\\n\")\n",
    "    data = data[1:14987] #不要部分をカット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# storing conll format data in multiple list\n",
    "document = []\n",
    "for sentence in data:#14988 sentences\n",
    "    \n",
    "    new_sentence_list = []\n",
    "    sentence_list = sentence.split(\"\\n\")\n",
    "    \n",
    "    for token in sentence_list:\n",
    "        token_list = token.split(\" \")\n",
    "        new_sentence_list.append(token_list)\n",
    "    document.append(new_sentence_list)\n",
    "    \n",
    "    # [[['-DOCSTART-', '-X-', '-X-', 'O']],\n",
    "    #  [['EU', 'NNP', 'B-NP', 'B-ORG'],\n",
    "    #   ['rejects', 'VBZ', 'B-VP', 'O'],\n",
    "    #   ['German', 'JJ', 'B-NP', 'B-MISC'],..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['B-ADVP', 'I-ADVP', 'I-ADJP', 'I-SBAR',\n",
    "#  'I-PP', 'B-LST', 'B-SBAR', 'B-NP',\n",
    "#  'B-ADJP', 'I-INTJ', 'B-VP', '-X-',\n",
    "#  'I-CONJP', 'B-PRT', 'B-CONJP', 'I-LST',\n",
    "#  'B-INTJ', 'B-PP', 'I-VP', 'O', 'I-NP']\n",
    "\n",
    "# I-B I-other cut\n",
    "#    I-I stay \n",
    "# B-I\n",
    "#   B-B B-oter cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_document = []\n",
    "\n",
    "for sentence in document:\n",
    "    sentence_list = []\n",
    "    \n",
    "    for i in range(len(sentence)-1):\n",
    "        sentence_list.append(sentence[i])\n",
    "        \n",
    "        if \"I-\" in sentence[i][2]:\n",
    "            if \"I-\" in sentence[i+1][2]:\n",
    "                pass\n",
    "            else: #B,other\n",
    "                sentence_list.append(-1)\n",
    "                \n",
    "        else: #B,other\n",
    "            if \"I-\" in sentence[i+1][2]:\n",
    "                pass\n",
    "            else: #B,other\n",
    "                sentence_list.append(-1)\n",
    "    \n",
    "    final_sentence_list = []\n",
    "    new_tokens_list = []\n",
    "    all_token = []\n",
    "    syntactic_tag = \"\"\n",
    "    bio_tag = []\n",
    "    \n",
    "    for token in sentence_list:\n",
    "        if token == -1 or token==sentence_list[-1]:\n",
    "            \n",
    "            new_tokens_list.append(tuple(all_token))\n",
    "            new_tokens_list.append(syntactic_tag)#タグ生成\n",
    "            new_tokens_list.append(tuple(bio_tag))\n",
    "            \n",
    "            final_sentence_list.append(new_tokens_list)\n",
    "            \n",
    "            new_tokens_list = []\n",
    "            all_token = []\n",
    "            syntactic_tag = \"\"\n",
    "            bio_tag = []\n",
    "        else:\n",
    "            all_token.append(token[0])\n",
    "            if syntactic_tag == \"\":\n",
    "                syntactic_tag=token[2]#タグ生成\n",
    "            bio_tag.append(token[3])\n",
    "            \n",
    "            \n",
    "    new_document.append(final_sentence_list)\n",
    "    \n",
    "    \n",
    "# new_document = []\n",
    "# for sentence in document:\n",
    "#     sentence_list = []\n",
    "    \n",
    "#     for i in range(len(sentence)-1):\n",
    "#         sentence_list.append(sentence[i])\n",
    "        \n",
    "#         if \"I-\" in sentence[i][2]:\n",
    "#             if \"I-\" in sentence[i+1][2]:\n",
    "#                 pass\n",
    "#             else: #B,other\n",
    "#                 sentence_list.append(-1)\n",
    "                \n",
    "#         else: #B,other\n",
    "#             if \"I-\" in sentence[i+1][2]:\n",
    "#                 pass\n",
    "#             else: #B,other\n",
    "#                 sentence_list.append(-1)\n",
    "    \n",
    "#     final_sentence_list = []\n",
    "#     new_tokens_list = []\n",
    "#     syntactic_tag = set()\n",
    "#     bio_tag = set()\n",
    "    \n",
    "#     for token in sentence_list:\n",
    "#         if token == -1 or token==sentence_list[-1]:\n",
    "            \n",
    "#             new_tokens_list.append(syntactic_tag)#タグ生成\n",
    "#             new_tokens_list.append(bio_tag)\n",
    "            \n",
    "#             final_sentence_list.append(new_tokens_list)\n",
    "#             new_tokens_list = []\n",
    "            \n",
    "#             syntactic_tag = set()\n",
    "#             bio_tag = set()\n",
    "#         else:\n",
    "#             syntactic_tag.add(token[2])#タグ生成\n",
    "#             bio_tag.add(token[3])\n",
    "            \n",
    "#             new_tokens_list.append([token[0],token[2],token[3]])\n",
    "        \n",
    "#     new_document.append(final_sentence_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[('EU',), 'B-NP', ('B-ORG',)],\n",
       "  [('rejects',), 'B-VP', ('O',)],\n",
       "  [('German', 'call'), 'B-NP', ('B-MISC', 'O')],\n",
       "  [('to', 'boycott'), 'B-VP', ('O', 'O')],\n",
       "  [('British', 'lamb'), 'B-NP', ('B-MISC', 'O')]],\n",
       " [[(), '', ()]]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_document[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# とりあえず，一ブロックに一単語のみ格納されているケースは削除\n",
    "# 2以上のケース　かつ　ブロック内のBOIタグ構造が完全に一致するものだけを交換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dictonary object with label as key and token as value\n",
    "label_token_dic = {}\n",
    "cnt = 0\n",
    "for sentence in new_document:\n",
    "    for block in sentence:\n",
    "        if len(block[0])!=1:# block内にトークン2以上の時のみ辞書登録\n",
    "            label = tuple(block[-2:])\n",
    "            cnt = cnt + 1\n",
    "            if label not in label_token_dic:\n",
    "                label_token_dic[label] = set()\n",
    "            label_token_dic[label].add(tuple(block[0]))\n",
    "        \n",
    "    # {('B-NP', ('B-MISC', 'O')): {('A$', '1'),\n",
    "    #   ('A$', '100'),\n",
    "    #   ('A$', '14,000'),\n",
    "    #   ('A$', '28,000'),...\n",
    "    \n",
    "    # label_token_dic.keys() でkey一覧取得可　個数：663\n",
    "    # cnt　に合計要素数格納　個数：41377"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making BI label list\n",
    "# BIを含むもののみ交換\n",
    "\n",
    "BI_label_list = []\n",
    "for label in list(label_token_dic.keys()):\n",
    "    NG_set = [{\"O\"},set(),{\"-X-\"},{\"O\",\"-X-\"}]\n",
    "    if set(label[1]) not in NG_set:\n",
    "        BI_label_list.append(label)\n",
    "        \n",
    "# [('B-NP', ('B-MISC', 'O')),\n",
    "#  ('B-NP', ('O', 'B-ORG', 'I-ORG')),\n",
    "#  ('B-NP', ('O', 'O', 'O', 'B-PER', 'I-PER')),..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sent2str(sentence_list):\n",
    "    return \" \".join([token_list[0] for token_list in sentence_list])\n",
    "\n",
    "def random_choice_token(token_set):\n",
    "    n = random.randint(0,len(token_set)-1)\n",
    "    token_list=list(token_set)\n",
    "    return token_list[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "\n",
    "\n",
    "def SyntacticDA_BI(n,filename):\n",
    "    data_f_path = \"data_augment/SyntacticDA_BI/\"+str(n)+\"_\"+filename+\".txt\"\n",
    "    memo_f_path = \"data_augment/SyntacticDA_BI/\"+str(n)+\"_\"+filename+\"_memo.txt\"\n",
    "    with open(data_f_path,\"w\") as data_f, open(memo_f_path,\"w\") as memo_f:\n",
    "        for now in range(n):\n",
    "            if now%10 == 0:\n",
    "                print('\\r%d / %d' %(now, n), end='')\n",
    "\n",
    "            x = random.randint(0,len(new_document)-1)\n",
    "            origin_sentence = new_document[x]\n",
    "            aug_sentence = []\n",
    "            cnt = 0\n",
    "\n",
    "            for token_list in origin_sentence:\n",
    "                token = token_list[0]\n",
    "                label = token_list[1]\n",
    "                origin_label = token_list[2]\n",
    "\n",
    "                if label in BI_label_list:\n",
    "                    if len(label_token_dic[label])!=1:\n",
    "                        token=random_choice_token(label_token_dic[label]-{token})\n",
    "                        cnt = cnt + 1\n",
    "                    \n",
    "\n",
    "                aug_sentence.append([token,origin_label])\n",
    "                data_f.write(\" \".join([token,origin_label])+\"\\n\")\n",
    "            data_f.write(\"\\n\")\n",
    "\n",
    "            memo_f.write(str(x)+\"\\t\"+Sent2str(new_document[x])+\"\\n\")\n",
    "            memo_f.write(\" \"*len(str(x))+\"\\t\"+Sent2str(aug_sentence)+\"\\n\")\n",
    "            memo_f.write(\"\\n\")\n",
    "        \n",
    "        memo_f.write(str(cnt)+\"sentences augmented!!!! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #train\n",
    "n = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1484990 / 1485000"
     ]
    }
   ],
   "source": [
    "SimpleDA_BI(n*1,\"train\")\n",
    "SimpleDA_BI(n*4,\"train\")\n",
    "SimpleDA_BI(n*9,\"train\")\n",
    "SimpleDA_BI(n*49,\"train\")\n",
    "SimpleDA_BI(n*99,\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## valid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_original/valid.txt', 'r', encoding='UTF-8') as f:\n",
    "    data = f.read()\n",
    "    data = data.split(\"\\n\\n\")\n",
    "    data = data[1:3466]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下同様\n",
    "document = []\n",
    "for sentence in data:#14988 sentences\n",
    "    \n",
    "    new_sentence_list = []\n",
    "    sentence_list = sentence.split(\"\\n\")\n",
    "    \n",
    "    for token in sentence_list:\n",
    "        token_list = token.split(\" \")\n",
    "        new_sentence_list.append(token_list)\n",
    "    document.append(new_sentence_list)\n",
    "\n",
    "new_document = []\n",
    "for sentence in document:\n",
    "    new_sentence_list = []\n",
    "    for token_list in sentence:\n",
    "        try:\n",
    "            new_token_list = [token_list[0],token_list[1]+\"_\"+token_list[3],token_list[3]]\n",
    "            new_sentence_list.append(new_token_list)\n",
    "        except:\n",
    "            pass\n",
    "    new_document.append(new_sentence_list)\n",
    "\n",
    "label_token_dic = {}\n",
    "for sentence in new_document:\n",
    "    for token_list in sentence:\n",
    "        label = token_list[1]\n",
    "        token = token_list[0]\n",
    "        if label not in label_token_dic:\n",
    "            label_token_dic[label] = set()\n",
    "        label_token_dic[label].add(token)\n",
    "\n",
    "        BI_label_list = [label for label in list(label_token_dic.keys()) if \"B-\" in label or \"I-\" in label]\n",
    "\n",
    "BI_label_list = [label for label in list(label_token_dic.keys()) if \"B-\" in label or \"I-\" in label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid\n",
    "n = 3500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346490 / 346500"
     ]
    }
   ],
   "source": [
    "SimpleDA_BI(n*1,\"valid\")\n",
    "SimpleDA_BI(n*4,\"valid\")\n",
    "SimpleDA_BI(n*9,\"valid\")\n",
    "SimpleDA_BI(n*49,\"valid\")\n",
    "SimpleDA_BI(n*99,\"valid\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
